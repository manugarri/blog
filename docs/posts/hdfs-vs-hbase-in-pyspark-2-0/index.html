<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>HDFS vs HBase in PySpark 2.0 | Manugarri&#39;s blog</title>


<link rel="stylesheet" href="/blog/css/style.css"/><link rel='stylesheet' href='https://manugarri.github.io/blog/css/custom.css'><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="\/\/matomo.example.com\/";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<noscript>
  <img src="//matomo.example.com/piwik.php?idsite=1&amp;rec=1" style="border:0" alt="" />
</noscript>


<section class="section">
  <div class="container">
    <nav class="nav">
      <div class="nav-left">
        <a class="nav-item" href="https://manugarri.github.io/blog/"><h1 class="title is-4">Manugarri&#39;s blog</h1></a>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile"><a class="level-item" aria-label="email" href='mailto:hola@manugarri.com' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
    <polyline points="22,6 12,13 2,6"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="github" href='https://github.com/manugarri' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="linkedin" href='https://linkedin.com/in/manuelgarridopena' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path stroke-width="1.8" d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="twitter" href='https://twitter.com/manugarri' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>

    <nav class="nav">
      

      
    </nav>

  </div>
</section>

<section class="section">
  <div class="container">
    <div class="subtitle tags is-6 is-pulled-right">
      
    </div>
    <h2 class="subtitle is-6">December 12, 2016</h2>
    <h1 class="title">HDFS vs HBase in PySpark 2.0</h1>
    
    <div class="content">
      

<p>It&rsquo;s been a challenging period at <a href="http://www.tribeclick.com/" target="_blank">Tribe</a>. In case you don&rsquo;t know, I am in charge of selecting and implementing the architecture for an ad Exchange with a focus on latency and performance.</p>

<p>As fun as it sounds, it is incredibly challenging, every decision in terms of tools not only affects the current performance, but given the sheer amount of data creates technical debt instantly (data migrations are always a pain, but when dealing with big data they are a big pain, hehe).</p>

<p>One of the decissions I had to take was how to store the final data. This decission is probably one of the most critical, as the structure and platform chosen will define how the data is read and how the data is written.</p>

<h5 id="interlude-adtech-stuff">Interlude - AdTech stuff</h5>

<p>Our platform receives a user that has clicked on an [Smartlink]() ad. Do you know those ads that when you click on them, they start redirecting to multiple urls? those are called smartlinks in the biz. What happens is basically your browser is bringing you to diferent Ad Agencies that are deciding whether to redirect you to one of their clients (advertisers), or to pass you along to another Ad Agency.</p>

<p>When the user lands on the final advertiser, and hopefully <em>converts</em> (conversion meaning the user has perform the action the advertiser is willing to pay for, whether is an app installation, or a purchase, or a subscription), the advertiser sends a <code>postback</code> (an HTTP post request to a prearranged endpoint) to the agency, so the agency can write it down and then charge the advertiser at the end of the month.</p>

<h5 id="back-to-dataland">Back to DataLand</h5>

<p>This basically means that when we receive a user, we create an opportunity object. This opportunity looks like</p>

<pre><code>{
&quot;created_at&quot;: 14805156161,
&quot;event_id&quot;: &quot;aoadwomawodi-awdawdd-awda&quot;,
&quot;user_agent&quot;: &quot;Internet Explorer 8&quot;,
&quot;ip&quot;: &quot;192.168.1.1&quot;,
&quot;device&quot;:{
  &quot;os&quot;: &quot;Android&quot;,
  &quot;model&quot;: &quot;Samsung&quot;
   },
}

</code></pre>

<p>Afterwards, once (if) we receive a postback, we have to update the opportunity to register the conversion event:</p>

<pre><code>{
&quot;created_at&quot;: 14805156161,
&quot;opportunity_id&quot;: &quot;aoadwomawodi-awdawdd-awda&quot;,
&quot;user_agent&quot;: &quot;Internet Explorer 8&quot;,
&quot;ip&quot;: &quot;192.168.1.1&quot;,
&quot;device&quot;:{
  &quot;os&quot;: &quot;Android&quot;,
  &quot;model&quot;: &quot;Samsung&quot;
   },
&quot;clicked&quot;: 1
}

</code></pre>

<p>Given than most opportunities don&rsquo;t convert (most ads aren&rsquo;t clicked, and most users do not convert), we need a system that after a reasonable amount of time, asumes that the conversion is never going to take place and register the conversion as failed for the opportunity:</p>

<pre><code>{
&quot;created_at&quot;: 14805156161,
&quot;opportunity_id&quot;: &quot;aoadwomawodi-awdawdd-awda&quot;,
&quot;user_agent&quot;: &quot;Internet Explorer 8&quot;,
&quot;ip&quot;: &quot;192.168.1.1&quot;,
&quot;device&quot;:{
  &quot;os&quot;: &quot;Android&quot;,
  &quot;model&quot;: &quot;Samsung&quot;
   },
&quot;clicked&quot;: 0
}
</code></pre>

<p>What this means from a technical point of view is that we are dealing with a massive amount of real time streaming data. This data needs to be stored in such a way that can be read relatively easy to train models.</p>

<p>But, at the same time, every single datapoint is going to be updated once.</p>

<p>This leaves us with the following options in terms of data storage.</p>

<ul>
<li><p><a href="http://hadoopilluminated.com/hadoop_illuminated/HDFS_Intro.html" target="_blank">HDFS</a> - The good ol&rsquo; reliable Hadoop Distributed File System. It just works. However, it is designed to build append only data lakes. In order to use HDFS, I would write the <em>opportunities</em> on HDFS, then the conversions on another path as a simple  list of <code>opportunity_id</code>, and then on train time, just do a massive join.</p></li>

<li><p><a href="https://www.tutorialspoint.com/hbase/hbase_overview.htm" target="_blank">HBASE</a> (on top of HDFS). HBase is another project of the Apache Foundation. In a nutshell, it brings CRUD to HDFS. It has a very interesting columnar structure and it creates indexes on top of HDFS. This allows users to update HDFS records, at the cost of slower read time.</p></li>
</ul>

<p>If I took this approach, things would be easier, wouldn&rsquo;t they? I could just write to HBASE the opportunities and then update the records once I received the postback.</p>

<p>Well, not really, I have found that HBASe has some cons as of 2016:</p>

<pre><code>- The documentation is lacking. And by documentation I mean updated tutorials. I have my fair share of experience installing tools in my machine, and setting up hbase on pseudo distributed mode on top of an existing zookeeper has been a challenge, mostly because of the so many different `hbase-site.xml` configurations that you can find around.

- Hbase does not support nested schemas, at least not officially. Given that I wanted to have flexibility of schemas, this was a big con for me.

- Pyspark HBase support is non existing. Period. I was baffled when I saw [this Commit](https://github.com/apache/spark/commit/a680562a6f87a03a00f71bad1c424267ae75c641) on the Spark project. On that commit, a contributor removes the examples of Pyspark connecting to hbase among others, and includes the commit message:

 *... Also, remove a couple of outdated examples. HBase has had Spark bindings for
</code></pre>

<p>a while and is even including them in the HBase distribution in the next
version, making the examples obsolete. The same applies to Cassandra, which
seems to have a proper Spark binding library already&hellip;*</p>

<pre><code> This is yet another example of Python being second hand citizens in the Spark World. As of 2016, **there is no official way of connecting pyspark to Hbase**. This is ridiculous. Hbase is a mature project (and a top level Apache Project, so is Spark), and adds a so much needed functionality to the distributed computing world.
</code></pre>

<p>Which brings me to&hellip;</p>

<h5 id="interlude-how-to-connect-pyspark-2-0-0-and-hbase">Interlude -  How to connect Pyspark 2.0.0 and HBase.</h5>

<p>Ain&rsquo;t easy. This is how I got HBASE read/write support in Pyspark 2.0.0:</p>

<ol>
<li>Set up <a href="https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm" target="_blank">Spark</a>, <a href="https://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm" target="_blank">HDFS</a>, <a href="https://www.tutorialspoint.com/hbase/hbase_installation.htm" target="_blank">Hbase</a> and (maybe) <a href="https://www.tutorialspoint.com/zookeeper/zookeeper_installation.htm" target="_blank">Zookeeper</a>. There are different ways of setting up each one of the services (standalone, distributed and pseudo distributed generally) and each use case will require a different one.</li>

<li><p>Create table hbase. Easy peasy, just do:</p>

<p><code>echo &quot;create 'TABLE_NAME', 'COLUMN_FAMILY'&quot; | hbase shell</code></p></li>

<li><p>Run Spark jobs the proper way.</p></li>
</ol>

<p>I found out that the only Hbase/Spark integration that worked for me <a href="https://github.com/hortonworks-spark/shc/issues/48" target="_blank">HortonWorks&rsquo; sch</a>. Even though they do not support spark 2.0.0 officially <a href="https://github.com/hortonworks-spark/shc/issues/48" target="_blank">right now</a>, the <a href="https://github.com/hortonworks-spark/shc/tree/branch-2.0" target="_blank">2.0 branch</a> works.</p>

<p>To use shc with pyspark you need to add the following arguments to the spark call (whether is <code>spark-submit</code> or <code>pyspark</code>:</p>

<p><code>--packages com.databricks:spark-avro_2.11:3.0.1,com.hortonworks:shc:1.0.0-2.0-s_2.11</code>, where <code>2.0</code> is your spark main version (2.0.0 on my case), <code>2.11</code> is your scala version.</p>

<p><code>--repositories http://repo.hortonworks.com/content/groups/public/</code>, adding the hortonworks repo so spark knows where to find the package.</p>

<p><code>--files $HBASE_HOME/conf/hbase-site.xml</code>, where <code>HBASE_HOME</code> is the path to your Hbase installation.</p>

<p>So an example <code>spark-submit</code> job would look like:</p>

<p><code>spark-submit --packages com.databricks:spark-avro_2.11:3.0.1,com.hortonworks:shc:1.0.0-2.0-s_2.11 --repositories http://repo.hortonworks.com/content/groups/public/  --files /usr/local/Hbase/conf/hbase-site.xml script.py arg1, arg2</code></p>

<p>In Pyspark, to write a dataframe to Hbase, you need to first define a catalog. They look like:</p>

<pre><code class="language-prettyprint">catalog = ''.join(&quot;&quot;&quot;{{
        &quot;table&quot;:{{&quot;namespace&quot;:&quot;default&quot;, &quot;name&quot;:&quot;{TABLE_NAME}&quot;}},
        &quot;rowkey&quot;:&quot;key&quot;,
        &quot;columns&quot;:{{
            &quot;key&quot;:{{&quot;cf&quot;:&quot;rowkey&quot;, &quot;col&quot;:&quot;key&quot;, &quot;type&quot;:&quot;string&quot;}},
            &quot;column1&quot;:{{&quot;cf&quot;:&quot;{COLUMN_FAMILY}&quot;, &quot;col&quot;:&quot;column1&quot;, &quot;type&quot;:&quot;string&quot;}},
            &quot;column2&quot;:{{&quot;cf&quot;:&quot;{COLUMN_FAMILY}&quot;, &quot;col&quot;:&quot;column2&quot;, &quot;type&quot;:&quot;string&quot;}}
            ... OTHER COLUMN DEFINITIONS HERE
        }}
        }}&quot;&quot;&quot;
</code></pre>

<p>With <code>TABLE_NAME</code> and <code>COLUMN_FAMILY</code> matching those defined when you create the table.</p>

<p>Now, once you have set up the Hbase data catalog you can write a dataframe to HBase like:</p>

<pre><code class="language-prettyprint">(df
  .write
  .options(catalog=catalog)
  .format('org.apache.spark.sql.execution.datasources.hbase')
  .save()
)
</code></pre>

<p>And read from Hbase like:</p>

<pre><code class="language-prettyprint">df_reloaded = (
          spark.read
          .options(catalog=catalog)
          .format('org.apache.spark.sql.execution.datasources.hbase')
          .load()
)
</code></pre>

<p>One caveat as of now, is that shc in pyspark only supports strings, so other data types need to be cast in an out of HBase.</p>

<h4 id="benchmarking">Benchmarking</h4>

<p>Given how critical this decision was for the project, I wanted to make sure I took the final decision based on the best benchmarks I could find. Well, I could not find any good benchmarks to use as a proxy for my use case, so I set up my own.</p>

<p>At a high level, these were the benchmark&rsquo;s tasks:</p>

<ol>
<li>Load opportunities to the final storage (Hbase or HDFS).</li>
<li>Load the conversions to the final storage (Hbase or HDFS)</li>
<li>Merge the opportunities and the clicks into a single dataframe. This meant doing a join on the HDFS only approach.</li>
<li>Data cleaning, casting tasks.</li>
<li>Multiple counts by different features with different cardinalities.</li>
<li>Multiple filters by different features with different cardinalities</li>
<li>Machine learning Pipeline training.</li>
</ol>

<p>The benchmarks were done with a large-is dataset (250MM records), on a testing 2 noders cluster.</p>

<h4 id="results">Results</h4>

<p>At the end, the results were pretty clear. Given how unique our use case is (one update per record), Hbase is not the solution. Regardless of the schemas tested (flat vs nested), the performance of a simple HDFS join outperformed HBase both in writes (obvious) and in complex reads (not so obvious).</p>

<p>Given how bad Hbase support in PySpark is right now combined with the superior performance of Hbase made it a no brainer for us. HDFS it is (was).</p>

<p>Thanks for reading! If you have any questions or comments on this article, please do not hesitate to reach out to me.</p>

      
      <div class="related">
</div>
      
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p></p>
    
      <p>Powered by <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/ribice/kiss">Kiss</a>.</p>
    
  </div>
</section>

</body>
</html>

